{
  "name": "OpenAI Prompt Engineering Guide",
  "schema_version": "latest",
  "metadata": {
    "models": {
      "gemini-pro": {
        "stream": true
      },
      "gpt-4": {
        "model": "gpt-4",
        "top_p": 1,
        "temperature": 0,
        "presence_penalty": 0,
        "frequency_penalty": 0
      }, 
      "gpt-3.5-turbo": {
        "model": "gpt-3.5-turbo",
        "top_p": 1,
        "temperature": 0,
        "presence_penalty": 0,
        "frequency_penalty": 0
      },
      "models/text-bison-001": {
        "topK": 40,
        "topP": 0.95,
        "model": "models/text-bison-001",
        "temperature": 0,
        "maxOutputTokens": 1024
      }
    },
    "parameters": {"original_prompt": "Write me a blog post on AI."}
  },
  "prompts": [
    {
      "name": "clear_instructions",
      "input": "You are given a user prompt: {{original_prompt}} \nYou will return the improved user prompt in triple quotes. Use a combination of the following tactics - use as many tactics as possible. \n\nOutput Format:\nStrategy: Write clear instructions\nTactic(s) Used\n<llist name of tactic(s) used>\n&nbsp;\n Original Prompt\n<insert original prompt here>\n&nbsp;\n Improved Prompt\n<insert improved prompt here>\n\nTactic: Include details in your query to get more relevant answers. Add 2-3 sentences of detail so it's very clear on the task needed.\nExamples: \nOriginal Prompt: Write code to calculate the Fibonacci sequence\nImproved Prompt: Write a TypeScript function to efficiently calculate the Fibonacci sequence. Comment the code liberally to explain what each piece does and why it's written that way.\n\nTactic: Ask the model to adopt a persona\nExamples: \nOriginal Prompt: Write me a thank you note. \nImproved Prompt: Write me a thank you note. Persona: When I ask for help to write something, you will reply with a document that contains at least one joke or playful comment in every paragraph.\n\nTactic: Use delimiters to clearly indicate distinct parts of the input.\nExamples: \nImproved Prompt: You will be provided with a thesis abstract and a suggested title for it. The thesis title should give the reader a good idea of the topic of the thesis but should also be eye-catching. If the title does not meet these criteria, suggest 5 alternatives. Abstract: \"\"\"insert abstract\"\" Title: \"\"\"insert title\"\"\"\n\nTactic: Specify the steps required to complete a task\nImproved Prompt: Use the following step-by-step instructions to respond to user inputs. User input: \"insert text\"\n\nTactic: Provide examples\nOriginal Prompt: Teach me about the ocean. \nImproved Prompt: Teach me about the ocean. Follow these examples: \"\"insert example explanations for multiple topics\"\"\n\nTactic: Specify desired length of output\nOriginal Prompt: Summarize the text. \nImproved Prompt:  Summarize the text delimited by triple quotes in about 50 words.",
      "metadata": {
        "model": {"name": "gpt-4"},
        "parameters": {},
        "remember_chat_context": false
      }
    },
    {
      "name": "provide_ref_text",
      "input": "You are given a user prompt: {{original_prompt}} \nYou will improve the user prompt by providing reference text. Language models can confidently invent fake answers, especially when asked about esoteric topics or for citations and URLs. In the same way that a sheet of notes can help a student do better on a test, providing reference text to these models can help in answering with fewer fabrications. Use a combination of the following tactics - use as many tactics as possible. \n\nOutput Format:\nStrategy: Provide reference text\nTactic(s) Used\n<llist name of tactic(s) used>\n&nbsp;\n Original Prompt\n<insert original prompt here>\n&nbsp;\n Improved Prompt\n<insert improved prompt here>\n\nTactic: Instruct the model to answer using a reference text. If we can provide a model with trusted information that is relevant to the current query, then we can instruct the model to use the provided information to compose its answer. Be as detailed and specific in the  reference text. \nExamples: \nOriginal Prompt: Answer this question. \nImproved Prompt: Answer this question using the given reference text in triple quotes. If answer is not in reference text, response 'I can't find the answer'. \"\"\"insert reference text\"\"\"\n\nTactic: Instruct the model to answer with citations from a reference text. If the input has been supplemented with relevant knowledge, it's straightforward to request that the model add citations to its answers by referencing passages from provided documents. Note that citations in the output can then be verified programmatically by string matching within the provided documents.\nExamples: \nImproved Prompt (for question prompts): \nYou will be provided with a document delimited by triple quotes and a question. Your task is to answer the question using only the provided document and to cite the passage(s) of the document used to answer the question. If the document does not contain the information needed to answer this question then simply write: \"Insufficient information.\" If an answer to the question is provided, it must be annotated with a citation. Use the following format for to cite relevant passages ({\"citation\": …}). \n\"\"\"<insert document here>\"\"\", Question: <insert question here>",
      "metadata": {
        "model": {"name": "gpt-4"},
        "parameters": {},
        "remember_chat_context": false
      }
    },
    {
      "name": "split_into_subtasks",
      "input": "You are given a user prompt: {{original_prompt}} \nYou will improve the user prompt by splitting complex tasks into simple subtasks. Use one of the tactics outlined - make it sure it is appropriate to the user prompt. \n\nOutput Format:\nStrategy: Split complex tasks into simple subtasks\nTactic(s) Used\n<llist name of tactic(s) used>\n&nbsp;\n Original Prompt\n<insert original prompt here>\n&nbsp;\n Improved Prompt\n<insert improved prompt here>\n\nTactic: Use intent classification to identify the most relevant instructions for a user query. Add in a list of categories to the prompt to help classify the right instructions based on those cateogires. \nExamples: \nOriginal Prompt: Help me fix my internet. \nImproved Prompt: You are given a user query: \"help me fix my internet\". Here is a potential list of relevant categories and sources to classify the query: <list of categories>\n\nTactic: For dialogue applications that require very long conversations, summarize or filter previous dialogue. \nExample: \nGiven user prompt is a long conversation, suggest this tactic.\n\nTactic: Summarize long documents piecewise and construct a full summary recursively. Since models have a fixed context length, they cannot be used to summarize a text longer than the context length minus the length of the generated summary in a single query. To summarize a very long document such as a book we can use a sequence of queries to summarize each section of the document. Section summaries can be concatenated and summarized producing summaries of summaries. This process can proceed recursively until an entire document is summarized. If it’s necessary to use information about earlier sections in order to make sense of later sections, then a further trick that can be useful is to include a running summary of the text that precedes any given point in the book while summarizing content at that point. \nExample: \nGiven the user prompt has a long document as context, suggest this tactic, outlining steps clearly. \n",
      "metadata": {
        "model": {"name": "gpt-4"},
        "parameters": {},
        "remember_chat_context": false
      }
    },
    {
      "name": "model_think_first",
      "input": "You are given a user prompt: {{original_prompt}} \nYou will improve the user prompt by giving the model time to think first. Pick one of the tactics. \n\nOutput Format:\nStrategy: Give the model time to 'think'\nTactic(s) Used\n<llist name of tactic(s) used>\n&nbsp;\n Original Prompt\n<insert original prompt here>\n&nbsp;\n Improved Prompt\n<insert improved prompt here>\n\nTactic: Instruct the model to work out its own solution before rushing to a conclusion\nExamples: \nOriginal Prompt: Determine if the student question is correct. \nImproved Prompt: You are given a student question. First work out your own solution to the problem. Then compare your solution to the student's solution and evaluate if the student's solution is correct or not. Don't decide if the student's solution is correct until you have done the problem yourself.\n\nTactic: Use inner monologue or a sequence of queries to hide the model's reasoning process. \nExamples: \nImproved Prompt (for question prompts): \nFollow these steps to answer the user queries.\n\nStep 1 - First work out your own solution to the problem. Don't rely on the student's solution since it may be incorrect. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 2 - Compare your solution to the student's solution and evaluate if the student's solution is correct or not. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 3 - If the student made a mistake, determine what hint you could give the student without giving away the answer. Enclose all your work for this step within triple quotes (\"\"\").\n\nStep 4 - If the student made a mistake, provide the hint from the previous step to the student (outside of triple quotes). Instead of writing \"Step 4 - ...\" write \"Hint:\".\n\nTactic: Ask the model if it missed anything on previous passes\nSuppose that we are using a model to list excerpts from a source which are relevant to a particular question. After listing each excerpt the model needs to determine if it should start writing another or if it should stop. If the source document is large, it is common for a model to stop too early and fail to list all relevant excerpts. In that case, better performance can often be obtained by prompting the model with followup queries to find any excerpts it missed on previous passes.",
      "metadata": {
        "model": {"name": "gpt-4"},
        "parameters": {},
        "remember_chat_context": false
      }
    },
    {
      "name": "systematic_testing",
      "input": "You are given a user prompt: {{original_prompt}} \nYou will improve the user prompt by testing changes systematically. Improving performance is easier if you can measure it. In some cases a modification to a prompt will achieve better performance on a few isolated examples but lead to worse overall performance on a more representative set of examples. Pick one of the tactics. \n\nOutput Format:\nStrategy: Test Changes Systematically\nTactic(s) Used\n<llist name of tactic(s) used>\n&nbsp;\n Original Prompt\n<insert original prompt here>\n&nbsp;\n Improved Prompt\n<insert improved prompt here>\n\nTactic: Evaluate model outputs with reference to gold-standard answers. When providing the pieces of information as gold-standard answers be very nuanced and specific - not high-level. \nExamples: \nOriginal Prompt: Tell me if this is the right answer to the question. Question: Who is Neil Armstrong? Answer: \"\"\"Neil Armstrong made history when he stepped off the lunar module, becoming the first person to walk on the moon.\"\"\"\nImproved Prompt: \nYou will be provided with text delimited by triple quotes that is supposed to be the answer to a question. Check if the following pieces of information are directly contained in the answer:\n\n- Neil Armstrong was the first person to walk on the moon.\n- The date Neil Armstrong first walked on the moon was July 21, 1969.\n\nFor each of these points perform the following steps:\n\n1 - Restate the point.\n2 - Provide a citation from the answer which is closest to this point.\n3 - Consider if someone reading the citation who doesn't know the topic could directly infer the point. Explain why or why not before making up your mind.\n4 - Write \"yes\" if the answer to 3 was yes, otherwise write \"no\".\n\nFinally, provide a count of how many \"yes\" answers there are. Provide this count as {\"count\": <insert count here>}.",
      "metadata": {
        "model": {"name": "gpt-4"},
        "parameters": {},
        "remember_chat_context": false
      }
    },
    {
      "name": "run_improved_prompt",
      "input": "{{improved_prompt}}",
      "metadata": {
        "model": {"name": "gpt-4"},
        "parameters": {"improved_prompt": ""},
        "remember_chat_context": false
      }
    }
  ]
}